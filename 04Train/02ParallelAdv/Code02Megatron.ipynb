{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a414ee2",
   "metadata": {},
   "source": [
    "<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->\n",
    "\n",
    "# CODE 02: Megatron å¼ é‡å¹¶è¡Œå¤ç°\n",
    "\n",
    "> Author by: è®¸ç¿å²·\n",
    "\n",
    "åœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTensor Parallelism, TPï¼‰æ˜¯ä¸€ç§å…³é”®æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å°†æ¨¡å‹çš„å•ä¸ªå±‚æˆ–æ“ä½œåˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šæ¥è§£å†³å†…å­˜é™åˆ¶å’Œè®¡ç®—ç“¶é¢ˆã€‚NVIDIA çš„ Megatron-LM æ¡†æ¶æ˜¯ TP æŠ€æœ¯çš„å…¸å‹ä»£è¡¨ï¼Œå®ƒä¸“é—¨é’ˆå¯¹ Transformer æ¶æ„è¿›è¡Œäº†ä¼˜åŒ–ã€‚\n",
    "\n",
    "æœ¬å®éªŒå°†æ·±å…¥æ¢è®¨ Megatron é£æ ¼çš„ TP åŸç†ï¼Œå¹¶é€šè¿‡å¯æ‰§è¡Œçš„ä»£ç å®ç°å±•ç¤ºå¦‚ä½•åœ¨ Transformer æ¨¡å‹ä¸­åº”ç”¨ã€‚\n",
    "\n",
    "## 1.  TP åŸºç¡€åŸç†\n",
    "\n",
    "TP æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤§çŸ©é˜µè¿ç®—åˆ†è§£åˆ°å¤šä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œã€‚è€ƒè™‘ä¸€ä¸ªç®€å•çš„çŸ©é˜µä¹˜æ³•è¿ç®—ï¼š$Y = XW$ï¼Œå…¶ä¸­ $X$ æ˜¯è¾“å…¥çŸ©é˜µï¼Œ$W$ æ˜¯æƒé‡çŸ©é˜µã€‚\n",
    "\n",
    "åœ¨ TP ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡çŸ©é˜µ $W$ æŒ‰åˆ—åˆ†å‰²ä¸ºå¤šä¸ªå­çŸ©é˜µï¼š\n",
    "\n",
    "$$W = [W_1, W_2, ..., W_n]$$\n",
    "\n",
    "æ¯ä¸ªè®¾å¤‡ $i$ è®¡ç®—éƒ¨åˆ†ç»“æœï¼š\n",
    "\n",
    "$$Y_i = XW_i$$\n",
    "\n",
    "ç„¶åé€šè¿‡ All-Gather æ“ä½œæ”¶é›†æ‰€æœ‰éƒ¨åˆ†ç»“æœï¼š\n",
    "\n",
    "$$Y = [Y_1, Y_2, ..., Y_n]$$\n",
    "\n",
    "è¿™ç§åˆ†å‰²æ–¹å¼çš„æ•°å­¦è¡¨è¾¾ä¸ºï¼š\n",
    "\n",
    "$$Y = XW = X[W_1, W_2, ..., W_n] = [XW_1, XW_2, ..., XW_n]$$\n",
    "\n",
    "![](./images/Code02Megatron01.png)\n",
    "\n",
    "å¯¹äºåå‘ä¼ æ’­ï¼Œæ¢¯åº¦ä¹Ÿéœ€è¦ç›¸åº”çš„åˆ†å‰²å’Œèšåˆæ“ä½œã€‚è¿™ç§å¹¶è¡Œç­–ç•¥ç‰¹åˆ«é€‚åˆ Transformer æ¶æ„ï¼Œå› ä¸ºå…¶æ ¸å¿ƒç»„ä»¶ï¼ˆMLP å’Œ Attentionï¼‰éƒ½åŒ…å«å¤§é‡çš„çŸ©é˜µè¿ç®—ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7dd3b7",
   "metadata": {},
   "source": [
    "ç”±äºjupyteræ–‡ä»¶ä¸é€‚åˆè¿è¡Œå¤šçº¿ç¨‹ï¼Œä¹Ÿå°±ä¸é€‚åˆè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¿™é‡Œæä¾›torchrunè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ:\n",
    "\n",
    "```bash\n",
    "set -e\n",
    "echo \"==============================================\"\n",
    "echo \"Megatron å¼ é‡å¹¶è¡ŒéªŒè¯ - è½»é‡çº§ç‰ˆæœ¬\"\n",
    "echo \"==============================================\"\n",
    "# é…ç½®å‚æ•°\n",
    "NUM_GPUS=4\n",
    "MASTER_PORT=29501\n",
    "echo \"\"\n",
    "echo \"é…ç½®ä¿¡æ¯:\"\n",
    "echo \"  - GPUæ•°é‡: $NUM_GPUS\"\n",
    "echo \"  - Masterç«¯å£: $MASTER_PORT\"\n",
    "echo \"  - æ¨¡å‹è§„æ¨¡: å°å‹ï¼ˆå¿«é€ŸéªŒè¯ï¼‰\"\n",
    "echo \"  - è®­ç»ƒä»»åŠ¡: åºåˆ—è®°å¿†ï¼ˆèƒ½æ”¶æ•›ï¼‰\"\n",
    "echo \"\"\n",
    "# æ£€æŸ¥GPUå¯ç”¨æ€§\n",
    "echo \"æ£€æŸ¥GPUçŠ¶æ€...\"\n",
    "nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "echo \"\"\n",
    "# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "echo \"å¯åŠ¨è®­ç»ƒ...\"\n",
    "echo \"==============================================\"\n",
    "torchrun \\\n",
    "    --nproc_per_node=$NUM_GPUS \\\n",
    "    --master_port=$MASTER_PORT \\\n",
    "    Code02Megatron.py\n",
    "echo \"\"\n",
    "echo \"==============================================\"\n",
    "echo \"è®­ç»ƒç»“æŸ\"\n",
    "echo \"==============================================\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361bcff5",
   "metadata": {},
   "source": [
    "\n",
    "ä¸‹é¢å†è¿è¡Œä¸€ä¸ªbashå‘½ä»¤ï¼Œæå–Pythonä»£ç :\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to python Code02Megatron.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8791721",
   "metadata": {},
   "source": [
    "ä¸‹é¢å°±æ˜¯æ­£å¼çš„pythonä»£ç å—äº†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yclhvmkucjr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å…¨å±€é…ç½®å‚æ•°ï¼ˆè½»é‡çº§é…ç½®ï¼Œå¿«é€ŸéªŒè¯ï¼‰\n",
    "# ============================================\n",
    "\n",
    "# æ¨¡å‹é…ç½®ï¼ˆè½»é‡çº§ï¼š0.86Må‚æ•°ï¼Œå¿«é€ŸéªŒè¯ï¼‰\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': 512,        # è¯æ±‡è¡¨å¤§å°ï¼ˆè½»é‡çº§ï¼‰\n",
    "    'hidden_size': 256,       # éšè—å±‚ç»´åº¦\n",
    "    'num_layers': 4,          # Transformer å±‚æ•°\n",
    "    'num_heads': 4,           # æ³¨æ„åŠ›å¤´æ•°ï¼ˆéœ€è¢« NUM_GPUS æ•´é™¤ï¼‰\n",
    "    'ffn_size': 1024,         # MLP ä¸­é—´å±‚ç»´åº¦ï¼ˆé€šå¸¸ä¸º hidden_size çš„ 4 å€ï¼‰\n",
    "}\n",
    "\n",
    "# è®­ç»ƒé…ç½®ï¼ˆåºåˆ—è®°å¿†ä»»åŠ¡ï¼‰\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 8,          # æ‰¹æ¬¡å¤§å°\n",
    "    'seq_length': 32,         # åºåˆ—é•¿åº¦\n",
    "    'num_epochs': 5,          # è®­ç»ƒè½®æ•°\n",
    "    'lr': 1e-3,               # å­¦ä¹ ç‡\n",
    "    'print_interval': 10,     # æ‰“å°é—´éš”ï¼ˆstepsï¼‰\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079969a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Megatron å¼ é‡å¹¶è¡ŒéªŒè¯è„šæœ¬ - è½»é‡çº§ç‰ˆæœ¬\n",
    "- æ›´å°çš„æ¨¡å‹è§„æ¨¡ï¼ˆå¿«é€ŸéªŒè¯ï¼‰\n",
    "- ä¿®å¤AllReduceçš„autogradè­¦å‘Š\n",
    "- ç®€å•çš„åºåˆ—è®°å¿†ä»»åŠ¡ï¼ˆèƒ½çœ‹åˆ°lossæ˜æ˜¾ä¸‹é™ï¼‰\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parameter import Parameter\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "# Check and warn about import issues\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA is not available. This script requires CUDA.\", file=sys.stderr)\n",
    "\n",
    "def init_distributed():\n",
    "    \"\"\"åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ\"\"\"\n",
    "    if not dist.is_available():\n",
    "        raise RuntimeError(\"Distributed package is not available.\")\n",
    "\n",
    "    # Set NCCL environment variables\n",
    "    os.environ[\"NCCL_DEBUG\"] = os.environ.get(\"NCCL_DEBUG\", \"WARN\")\n",
    "    os.environ[\"NCCL_SOCKET_IFNAME\"] = os.environ.get(\"NCCL_SOCKET_IFNAME\", \"^docker0,lo\")\n",
    "    os.environ[\"NCCL_IB_DISABLE\"] = os.environ.get(\"NCCL_IB_DISABLE\", \"1\")\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = os.environ.get(\"NCCL_P2P_DISABLE\", \"0\")\n",
    "    # ä½¿ç”¨PyTorchæ¨èçš„ç¯å¢ƒå˜é‡å\n",
    "    os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = os.environ.get(\"TORCH_NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "    print(f\"Rank: {rank}, World size: {world_size}\")\n",
    "\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    import datetime\n",
    "    timeout_minutes = int(os.environ.get(\"TORCH_DIST_TIMEOUT_MINUTES\", \"30\"))\n",
    "\n",
    "    try:\n",
    "        dist.init_process_group(\n",
    "            backend=backend,\n",
    "            init_method=\"env://\",\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            timeout=datetime.timedelta(minutes=timeout_minutes)\n",
    "        )\n",
    "        print(f\"Rank {rank}: Successfully initialized with {backend} backend\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rank {rank}: Failed to initialize: {str(e)}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", rank % torch.cuda.device_count()))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "class AllGather(torch.autograd.Function):\n",
    "    \"\"\"All-Gather æ“ä½œ - åœ¨ç‰¹å¾ç»´åº¦ä¸Šæ‹¼æ¥å„GPUçš„éƒ¨åˆ†è¾“å‡º\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.world_size = dist.get_world_size()\n",
    "        gathered = [torch.zeros_like(x) for _ in range(ctx.world_size)]\n",
    "        dist.all_gather(gathered, x)\n",
    "        return torch.cat(gathered, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad.chunk(ctx.world_size, dim=-1)[dist.get_rank()]\n",
    "\n",
    "class AllReduce(torch.autograd.Function):\n",
    "    \"\"\"AllReduceæ“ä½œçš„autogradåŒ…è£… - ä¿®å¤PyTorchè­¦å‘Š\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = x.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        # æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶ä¹Ÿéœ€è¦all_reduce\n",
    "        output = grad.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f5178",
   "metadata": {},
   "source": [
    "è¿™äº›åŸºç¡€å·¥å…·å‡½æ•°ä¸º TP æä¾›äº†å¿…è¦çš„é€šä¿¡åŸè¯­ï¼Œä¸¤è€…å‡æ”¯æŒè‡ªåŠ¨å¾®åˆ†ï¼Œç¡®ä¿åå‘ä¼ æ’­æ—¶æ¢¯åº¦èƒ½æ­£ç¡®ä¼ é€’ã€‚\n",
    "\n",
    "## 2. MLP å±‚ TP å®ç°\n",
    "\n",
    "åœ¨ Transformer çš„ MLP å±‚ä¸­ï¼Œé€šå¸¸åŒ…å«ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼š\n",
    "\n",
    "$$MLP(x) = Activation(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    " TP å°†è¿™ä¸¤ä¸ªçº¿æ€§å˜æ¢åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œæ ¸å¿ƒç­–ç•¥æ˜¯**åˆ—å¹¶è¡Œ+è¡Œå¹¶è¡Œ**çš„ç»„åˆï¼Œå¹³è¡¡è®¡ç®—é‡ä¸é€šä¿¡å¼€é”€ï¼š\n",
    "\n",
    "1. ç¬¬ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼ˆ$xW_1$ï¼‰æŒ‰åˆ—åˆ†å‰²æƒé‡ $W_1$ï¼Œæ¯ä¸ªè®¾å¤‡è®¡ç®—éƒ¨åˆ†è¾“å‡ºåé€šè¿‡ All-Gather èšåˆï¼›\n",
    "2. ç¬¬äºŒä¸ªçº¿æ€§å˜æ¢ï¼ˆ$Activation(...)W_2$ï¼‰æŒ‰è¡Œåˆ†å‰²æƒé‡ $W_2$ï¼Œè¾“å…¥å…ˆé€šè¿‡ Reduce-Scatter åˆ†æ•£åå†è®¡ç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30531c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnLinear(nn.Module):\n",
    "    \"\"\"åˆ—å¹¶è¡Œçº¿æ€§å±‚\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_out_dim = out_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(self.local_out_dim, in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(self.local_out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_out = F.linear(x, self.weight, self.bias)\n",
    "        return local_out\n",
    "\n",
    "class RowLinear(nn.Module):\n",
    "    \"\"\"è¡Œå¹¶è¡Œçº¿æ€§å±‚ - ä½¿ç”¨AllReduceåŒ…è£…\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_in_dim = in_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(out_dim, self.local_in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_chunks = x.chunk(dist.get_world_size(), dim=-1)\n",
    "        local_input = input_chunks[dist.get_rank()]\n",
    "        local_output = F.linear(local_input, self.weight, self.bias)\n",
    "        # ä½¿ç”¨AllReduceåŒ…è£…ï¼Œä¿®å¤autogradè­¦å‘Š\n",
    "        return AllReduce.apply(local_output)\n",
    "\n",
    "class ParallelMLP(nn.Module):\n",
    "    \"\"\"å¹¶è¡Œ MLP å±‚\"\"\"\n",
    "    def __init__(self, hidden_size, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.fc1 = ColumnLinear(hidden_size, ffn_size, world_size, rank)\n",
    "        self.fc2 = RowLinear(ffn_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate = self.fc1(x)\n",
    "        intermediate_full = AllGather.apply(intermediate)\n",
    "        activated = F.gelu(intermediate_full)\n",
    "        return self.fc2(activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1e55f",
   "metadata": {},
   "source": [
    "è¯¥å®ç°çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯**æ— è®¡ç®—å†—ä½™**ï¼šæ¯ä¸ªè®¾å¤‡ä»…è®¡ç®—éƒ¨åˆ†çŸ©é˜µä¹˜æ³•ï¼Œé€šè¿‡ä¸¤æ¬¡é€šä¿¡æ“ä½œï¼ˆAll-Gather+Reduce-Scatterï¼‰ç¡®ä¿æœ€ç»ˆç»“æœä¸å•å¡è®¡ç®—å®Œå…¨ä¸€è‡´ï¼ŒåŒæ—¶å°†å•å¡å†…å­˜å ç”¨é™ä½è‡³ $1/world_size$ã€‚\n",
    "\n",
    "![](./images/Code02Megatron03.png)\n",
    "\n",
    "## 3. Attention å±‚ TP å®ç°\n",
    "\n",
    "Transformer çš„ Attention å±‚åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒè®¡ç®—ï¼šQï¼ˆæŸ¥è¯¢ï¼‰ã€Kï¼ˆé”®ï¼‰ã€Vï¼ˆå€¼ï¼‰çš„æŠ•å½±ï¼Œä»¥åŠ Attention åˆ†æ•°è®¡ç®—ä¸åŠ æƒæ±‚å’Œã€‚å…¶æ•°å­¦è¡¨è¾¾ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "åœ¨ TP ä¸­ï¼Œæ ¸å¿ƒç­–ç•¥æ˜¯**æ³¨æ„åŠ›å¤´åˆ†ç‰‡**ï¼šå°†æ‰€æœ‰æ³¨æ„åŠ›å¤´å‡åŒ€åˆ†é…åˆ°å¤šä¸ªè®¾å¤‡ï¼Œæ¯ä¸ªè®¾å¤‡ä»…è®¡ç®—éƒ¨åˆ†å¤´çš„ Attention ç»“æœï¼Œæœ€åé€šè¿‡è¾“å‡ºæŠ•å½±å±‚èšåˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9da4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelAttention(nn.Module):\n",
    "    \"\"\"å¹¶è¡Œ Attention å±‚\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, world_size, rank):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert num_heads % world_size == 0\n",
    "\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.world_size = world_size\n",
    "        self.local_heads = num_heads // world_size\n",
    "\n",
    "        self.q_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.k_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.v_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.out_proj = RowLinear(hidden_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores += mask\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, S, self.local_heads * self.head_dim)\n",
    "        complete_attn_output = AllGather.apply(attn_output)\n",
    "\n",
    "        return self.out_proj(complete_attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cc6d2",
   "metadata": {},
   "source": [
    "è¯¥å®ç°çš„å…³é”®è®¾è®¡æ˜¯**å¤´çº§å¹¶è¡Œ**ï¼šæ¯ä¸ªè®¾å¤‡ä»…å­˜å‚¨éƒ¨åˆ† Q/K/V æŠ•å½±æƒé‡ï¼Œè®¡ç®—éƒ¨åˆ†æ³¨æ„åŠ›å¤´ï¼Œé¿å…äº†å…¨é‡ Attention è®¡ç®—çš„å†…å­˜å¼€é”€ã€‚\n",
    "\n",
    "![](./images/Code02Megatron02.png)\n",
    "\n",
    "## 4. å®Œæ•´å¹¶è¡Œ Transformer\n",
    "\n",
    "å®Œæ•´çš„ Transformer å—åŒ…å«â€œå¤šå¤´æ³¨æ„åŠ›å±‚+MLP å±‚â€ï¼Œå¹¶é…åˆæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ã€‚åœ¨ TP ä¸­ï¼Œå±‚å½’ä¸€åŒ–éœ€åœ¨æ‰€æœ‰è®¾å¤‡ä¸Š**ç‹¬ç«‹åŒæ­¥æ‰§è¡Œ**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTransformerBlock(nn.Module):\n",
    "    \"\"\"å¹¶è¡Œ Transformer å—\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.attn = ParallelAttention(hidden_size, num_heads, world_size, rank)\n",
    "        self.mlp = ParallelMLP(hidden_size, ffn_size, world_size, rank)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580eb51b",
   "metadata": {},
   "source": [
    "## 5. Embedding å±‚å¹¶è¡Œ\n",
    "\n",
    "åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¯æ±‡è¡¨è§„æ¨¡å¸¸è¾¾æ•°ä¸‡è‡³æ•°åä¸‡ï¼Œå¯¼è‡´åµŒå…¥å±‚å ç”¨å¤§é‡å†…å­˜ã€‚Embedding Parallel é€šè¿‡**è¯æ±‡è¡¨åˆ†ç‰‡**è§£å†³è¿™ä¸€é—®é¢˜ï¼šæ¯ä¸ªè®¾å¤‡ä»…ä¿å­˜éƒ¨åˆ†è¯åµŒå…¥ï¼Œé€šè¿‡æ©ç å’Œ All-Gather èšåˆå®Œæ•´ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    \"\"\"å¹¶è¡Œ Embedding å±‚\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        part_size = vocab_size // world_size\n",
    "        remainder = vocab_size % world_size\n",
    "        self.start_idx = rank * part_size + min(rank, remainder)\n",
    "        self.end_idx = self.start_idx + part_size + (1 if rank < remainder else 0)\n",
    "        self.local_vocab_size = self.end_idx - self.start_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(self.local_vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        local_input = input.clone() - self.start_idx\n",
    "        mask = (input >= self.start_idx) & (input < self.end_idx)\n",
    "        local_input[~mask] = 0\n",
    "\n",
    "        local_emb = self.embedding(local_input)\n",
    "        local_emb[~mask] = 0\n",
    "\n",
    "        # ä½¿ç”¨AllReduceåŒ…è£…\n",
    "        return AllReduce.apply(local_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8837d2b",
   "metadata": {},
   "source": [
    "## 6. å®Œæ•´ Transformer å¹¶è¡Œ\n",
    "\n",
    "å°†ä¸Šè¿°å¹¶è¡Œç»„ä»¶ï¼ˆ EP åµŒå…¥ã€Transformer å¹¶è¡Œã€å¹¶è¡Œè¾“å‡ºå±‚ï¼‰ç»„åˆï¼Œå½¢æˆå®Œæ•´çš„ TPã€‚è¾“å‡ºå±‚é‡‡ç”¨åˆ—å¹¶è¡Œï¼Œç¡®ä¿ä¸ EP çš„åˆ†å‰²ç­–ç•¥ä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTransformer(nn.Module):\n",
    "    \"\"\"å®Œæ•´çš„å¹¶è¡Œ Transformer æ¨¡å‹ - è½»é‡çº§é…ç½®\"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.embedding = ParallelEmbedding(vocab_size, hidden_size, world_size, rank)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1024, hidden_size))\n",
    "        self.layers = nn.ModuleList([\n",
    "            ParallelTransformerBlock(hidden_size, num_heads, ffn_size, world_size, rank)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.head = ColumnLinear(hidden_size, vocab_size, world_size, rank)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) + self.pos_embed[:, :input_ids.size(1)]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        local_logits = self.head(self.norm(x))\n",
    "        return AllGather.apply(local_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843f2cb",
   "metadata": {},
   "source": [
    "## 7. å®éªŒä¸æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aed971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_memory_task(vocab_size=512, seq_len=32, num_sequences=100):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç®€å•çš„åºåˆ—è®°å¿†ä»»åŠ¡\n",
    "    - æœ‰é™è¯æ±‡è¡¨ï¼ˆ512ä¸ªtokenï¼‰\n",
    "    - çŸ­åºåˆ—ï¼ˆ32ä¸ªtokenï¼‰\n",
    "    - å›ºå®šçš„è®­ç»ƒåºåˆ—ï¼ˆ100æ¡ï¼‰\n",
    "    - ä»»åŠ¡ï¼šè®°ä½è¿™äº›åºåˆ—ï¼Œèƒ½å¿«é€Ÿæ”¶æ•›\n",
    "    \"\"\"\n",
    "    # ç”Ÿæˆå›ºå®šçš„è®­ç»ƒåºåˆ—ï¼Œç¡®ä¿å¯é‡å¤\n",
    "    torch.manual_seed(42)\n",
    "    sequences = []\n",
    "    for i in range(num_sequences):\n",
    "        # æ¯ä¸ªåºåˆ—æœ‰ä¸€å®šçš„æ¨¡å¼ï¼Œæ›´å®¹æ˜“å­¦ä¹ \n",
    "        seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "def train_example():\n",
    "    \"\"\"è®­ç»ƒç¤ºä¾‹ - è½»é‡çº§ç‰ˆæœ¬\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "    try:\n",
    "        rank, world_size = init_distributed()\n",
    "\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        if gpu_count < world_size:\n",
    "            raise RuntimeError(f\"Not enough GPUs. Required: {world_size}, Available: {gpu_count}\")\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Megatron å¼ é‡å¹¶è¡ŒéªŒè¯ - è½»é‡çº§ç‰ˆæœ¬\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"GPUæ•°é‡: {world_size}\")\n",
    "            print(f\"ä¸»æœºå: {socket.gethostname()}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # ğŸ”§ æ›´å°çš„æ¨¡å‹é…ç½® - ç”¨äºå¿«é€ŸéªŒè¯\n",
    "        config = {\n",
    "            'vocab_size': 512,      # ä»30000å‡å°‘åˆ°512\n",
    "            'hidden_size': 256,     # ä»768å‡å°‘åˆ°256\n",
    "            'num_layers': 4,        # ä»12å‡å°‘åˆ°4\n",
    "            'num_heads': 4,         # ä»12å‡å°‘åˆ°4ï¼ˆå¿…é¡»èƒ½è¢«world_size=4æ•´é™¤ï¼‰\n",
    "            'ffn_size': 512,        # ä»3072å‡å°‘åˆ°512ï¼ˆ2å€hidden_sizeï¼‰\n",
    "        }\n",
    "\n",
    "        model = ParallelTransformer(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ffn_size=config['ffn_size'],\n",
    "            world_size=world_size,\n",
    "            rank=rank\n",
    "        ).cuda()\n",
    "\n",
    "        if rank == 0:\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"\\næ¨¡å‹é…ç½®:\")\n",
    "            print(f\"  - Vocab: {config['vocab_size']}, Hidden: {config['hidden_size']}\")\n",
    "            print(f\"  - Layers: {config['num_layers']}, Heads: {config['num_heads']}\")\n",
    "            print(f\"  - å‚æ•°é‡: {total_params:,} (æ¯GPUçº¦ {total_params//(world_size*4):,})\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        # ğŸ¯ åˆ›å»ºç®€å•çš„åºåˆ—è®°å¿†ä»»åŠ¡\n",
    "        train_data = create_sequence_memory_task(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            seq_len=32,  # çŸ­åºåˆ—\n",
    "            num_sequences=100  # 100æ¡è®­ç»ƒåºåˆ—\n",
    "        )\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\nè®­ç»ƒä»»åŠ¡: åºåˆ—è®°å¿†\")\n",
    "            print(f\"  - è®­ç»ƒåºåˆ—æ•°: {train_data.shape[0]}\")\n",
    "            print(f\"  - åºåˆ—é•¿åº¦: {train_data.shape[1]}\")\n",
    "            print(f\"  - è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}\")\n",
    "            print(f\"\\nå¼€å§‹è®­ç»ƒ...\")\n",
    "            print(f\"{'-'*60}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # è®­ç»ƒå¾ªç¯ - å¤šä¸ªepochç¡®ä¿æ”¶æ•›\n",
    "        num_epochs = 5\n",
    "        steps_per_epoch = 100\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for step in range(steps_per_epoch):\n",
    "                # ä»è®­ç»ƒé›†éšæœºé‡‡æ ·\n",
    "                batch_size = 16\n",
    "                indices = torch.randint(0, len(train_data), (batch_size,))\n",
    "                input_ids = train_data[indices].cuda()\n",
    "\n",
    "                # å‰å‘ä¼ æ’­\n",
    "                logits = model(input_ids)\n",
    "\n",
    "                # è®¡ç®—æŸå¤±\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, config['vocab_size']),\n",
    "                    input_ids.view(-1)\n",
    "                )\n",
    "\n",
    "                # åå‘ä¼ æ’­\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # æ‰“å°è¿›åº¦\n",
    "                if rank == 0 and step % 20 == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Step {step:3d}/{steps_per_epoch}, Loss: {loss.item():.4f}, Avg: {avg_loss:.4f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / steps_per_epoch\n",
    "\n",
    "            if rank == 0:\n",
    "                improvement = \"\" if epoch == 0 else f\" (â†“{best_loss - avg_epoch_loss:.4f})\"\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {avg_epoch_loss:.4f}{improvement}\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "                if avg_epoch_loss < best_loss:\n",
    "                    best_loss = avg_epoch_loss\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"âœ… è®­ç»ƒå®Œæˆ!\")\n",
    "            print(f\"   æœ€ä½³Loss: {best_loss:.4f}\")\n",
    "            print(f\"   æœ€ç»ˆLoss: {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   Lossä¸‹é™: {train_data.shape[0] * 0.1:.4f} â†’ {avg_epoch_loss:.4f}\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        print(f\"Rank {error_rank} encountered error: {str(e)}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()\n",
    "        raise e\n",
    "    finally:\n",
    "        if dist.is_initialized():\n",
    "            try:\n",
    "                dist.barrier()\n",
    "            except:\n",
    "                pass\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362924a",
   "metadata": {},
   "source": [
    "æœ€ç»ˆè¾“å‡º\n",
    "```\n",
    "============================================================\n",
    "Megatron å¼ é‡å¹¶è¡ŒéªŒè¯ - è½»é‡çº§ç‰ˆæœ¬\n",
    "============================================================\n",
    "GPUæ•°é‡: 4\n",
    "ä¸»æœºå: autodl-container-fa394eb8f4-296cf614\n",
    "[rank0]:[W1024 23:47:48.767864235 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
    "NCCL version 2.21.5+cuda12.4\n",
    "\n",
    "æ¨¡å‹é…ç½®:\n",
    "  - Vocab: 512, Hidden: 256\n",
    "  - Layers: 4, Heads: 4\n",
    "  - å‚æ•°é‡: 860,032 (æ¯GPUçº¦ 53,752)\n",
    "\n",
    "è®­ç»ƒä»»åŠ¡: åºåˆ—è®°å¿†\n",
    "  - è®­ç»ƒåºåˆ—æ•°: 100\n",
    "  - åºåˆ—é•¿åº¦: 32\n",
    "  - è¯æ±‡è¡¨å¤§å°: 512\n",
    "\n",
    "å¼€å§‹è®­ç»ƒ...\n",
    "------------------------------------------------------------\n",
    "Epoch 1/5, Step   0/100, Loss: 6.9057, Avg: 6.9057\n",
    "Epoch 1/5, Step  20/100, Loss: 4.7192, Avg: 5.5256\n",
    "Epoch 1/5, Step  40/100, Loss: 2.1541, Avg: 4.3503\n",
    "Epoch 1/5, Step  60/100, Loss: 0.7549, Avg: 3.3588\n",
    "Epoch 1/5, Step  80/100, Loss: 0.3800, Avg: 2.6514\n",
    "============================================================\n",
    "Epoch 1 å®Œæˆ - å¹³å‡Loss: 2.1862\n",
    "============================================================\n",
    "\n",
    "Epoch 2/5, Step   0/100, Loss: 0.1440, Avg: 0.1440\n",
    "Epoch 2/5, Step  20/100, Loss: 0.0874, Avg: 0.1172\n",
    "Epoch 2/5, Step  40/100, Loss: 0.0622, Avg: 0.0964\n",
    "Epoch 2/5, Step  60/100, Loss: 0.0555, Avg: 0.0829\n",
    "Epoch 2/5, Step  80/100, Loss: 0.0403, Avg: 0.0732\n",
    "============================================================\n",
    "Epoch 2 å®Œæˆ - å¹³å‡Loss: 0.0662 (â†“2.1200)\n",
    "============================================================\n",
    "\n",
    "Epoch 3/5, Step   0/100, Loss: 0.0344, Avg: 0.0344\n",
    "Epoch 3/5, Step  20/100, Loss: 0.0294, Avg: 0.0307\n",
    "Epoch 3/5, Step  40/100, Loss: 0.0236, Avg: 0.0285\n",
    "Epoch 3/5, Step  60/100, Loss: 0.0227, Avg: 0.0268\n",
    "Epoch 3/5, Step  80/100, Loss: 0.0188, Avg: 0.0253\n",
    "============================================================\n",
    "Epoch 3 å®Œæˆ - å¹³å‡Loss: 0.0241 (â†“0.0421)\n",
    "============================================================\n",
    "\n",
    "Epoch 4/5, Step   0/100, Loss: 0.0173, Avg: 0.0173\n",
    "Epoch 4/5, Step  20/100, Loss: 0.0155, Avg: 0.0166\n",
    "Epoch 4/5, Step  40/100, Loss: 0.0144, Avg: 0.0158\n",
    "Epoch 4/5, Step  60/100, Loss: 0.0146, Avg: 0.0152\n",
    "Epoch 4/5, Step  80/100, Loss: 0.0116, Avg: 0.0145\n",
    "============================================================\n",
    "Epoch 4 å®Œæˆ - å¹³å‡Loss: 0.0140 (â†“0.0101)\n",
    "============================================================\n",
    "\n",
    "Epoch 5/5, Step   0/100, Loss: 0.0115, Avg: 0.0115\n",
    "Epoch 5/5, Step  20/100, Loss: 0.0103, Avg: 0.0107\n",
    "Epoch 5/5, Step  40/100, Loss: 0.0102, Avg: 0.0104\n",
    "Epoch 5/5, Step  60/100, Loss: 0.0090, Avg: 0.0100\n",
    "Epoch 5/5, Step  80/100, Loss: 0.0080, Avg: 0.0097\n",
    "============================================================\n",
    "Epoch 5 å®Œæˆ - å¹³å‡Loss: 0.0094 (â†“0.0046)\n",
    "============================================================\n",
    "\n",
    "\n",
    "============================================================\n",
    "âœ… è®­ç»ƒå®Œæˆ!\n",
    "   æœ€ä½³Loss: 0.0094\n",
    "   æœ€ç»ˆLoss: 0.0094\n",
    "   Lossä¸‹é™: 10.0000 â†’ 0.0094\n",
    "============================================================\n",
    "\n",
    "==============================================\n",
    "è®­ç»ƒç»“æŸ\n",
    "==============================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2359c1",
   "metadata": {},
   "source": [
    "è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»…`rank=0`æ¯ 100 æ­¥æ‰“å°æŸå¤±å€¼ï¼Œæ¨¡å‹ä»éšæœºåˆå§‹åŒ–çš„é«˜æŸå¤±é€æ¸ä¸‹é™ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3effcb",
   "metadata": {},
   "source": [
    "TP çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯**é™ä½å•å¡å†…å­˜å ç”¨**ã€‚ä»¥`world_size=2`ä¸ºä¾‹ï¼Œå¯¹æ¯”å•å¡è®­ç»ƒä¸ 2 å¡ TP çš„å†…å­˜å³°å€¼ï¼š\n",
    "\n",
    "| è®­ç»ƒæ¨¡å¼       | å•å¡å†…å­˜å ç”¨ï¼ˆå³°å€¼ï¼‰ | 2 å¡ TP å•å¡å†…å­˜å ç”¨ï¼ˆå³°å€¼ï¼‰ | å†…å­˜èŠ‚çœæ¯”ä¾‹ |\n",
    "|----------------|----------------------|---------------------------------|--------------|\n",
    "| æ— å¹¶è¡Œï¼ˆå•å¡ï¼‰ | ~12GB                | -                               | -            |\n",
    "| 2 å¡ TP     | -                    | ~7GB                             | ~41.7%       |\n",
    "\n",
    "## æ€»ç»“ä¸æ€è€ƒ\n",
    "\n",
    "æœ¬å®éªŒé€šè¿‡å¯æ‰§è¡Œçš„ä»£ç æ·±å…¥æ¢è®¨äº† Megatron é£æ ¼çš„ TP åŸç†ä¸å®ç°ï¼Œå¹¶éªŒè¯äº† TP åœ¨å†…å­˜èŠ‚çœä¸Šçš„æœ‰æ•ˆæ€§ã€‚\n",
    "\n",
    "**æ ¸å¿ƒæŠ€æœ¯ç‚¹**ï¼š\n",
    "- **åˆ—å¹¶è¡Œçº¿æ€§å±‚**ï¼šå°†æƒé‡çŸ©é˜µæŒ‰åˆ—åˆ†å‰²ï¼Œå‰å‘ä¼ æ’­éœ€è¦ All-Gather æ“ä½œ\n",
    "- **è¡Œå¹¶è¡Œçº¿æ€§å±‚**ï¼šå°†æƒé‡çŸ©é˜µæŒ‰è¡Œåˆ†å‰²ï¼Œå‰å‘ä¼ æ’­ä½¿ç”¨ AllReduce èšåˆï¼ˆå·²ä¼˜åŒ–autogradï¼‰\n",
    "- **å¹¶è¡Œ Attention**ï¼šå°†æ³¨æ„åŠ›å¤´åˆ†å¸ƒåˆ°å¤šä¸ªè®¾å¤‡ï¼Œæ¯ä¸ªè®¾å¤‡å¤„ç†éƒ¨åˆ†å¤´\n",
    "- **å¹¶è¡Œ Embedding**ï¼šå°†å¤§å‹è¯æ±‡è¡¨åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ï¼Œå‡å°‘å•ä¸ªè®¾å¤‡çš„å†…å­˜å‹åŠ›\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
