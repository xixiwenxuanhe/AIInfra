{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a414ee2",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 02: Megatron 张量并行复现\n",
    "\n",
    "> Author by: 许灿岷\n",
    "\n",
    "在大模型训练中，张量并行（Tensor Parallelism, TP）是一种关键技术，它通过将模型的单个层或操作分布在多个设备上来解决内存限制和计算瓶颈。NVIDIA 的 Megatron-LM 框架是 TP 技术的典型代表，它专门针对 Transformer 架构进行了优化。\n",
    "\n",
    "本实验将深入探讨 Megatron 风格的 TP 原理，并通过可执行的代码实现展示如何在 Transformer 模型中应用。\n",
    "\n",
    "## 1.  TP 基础原理\n",
    "\n",
    "TP 核心思想是将大矩阵运算分解到多个设备上执行。考虑一个简单的矩阵乘法运算：$Y = XW$，其中 $X$ 是输入矩阵，$W$ 是权重矩阵。\n",
    "\n",
    "在 TP 中，我们将权重矩阵 $W$ 按列分割为多个子矩阵：\n",
    "\n",
    "$$W = [W_1, W_2, ..., W_n]$$\n",
    "\n",
    "每个设备 $i$ 计算部分结果：\n",
    "\n",
    "$$Y_i = XW_i$$\n",
    "\n",
    "然后通过 All-Gather 操作收集所有部分结果：\n",
    "\n",
    "$$Y = [Y_1, Y_2, ..., Y_n]$$\n",
    "\n",
    "这种分割方式的数学表达为：\n",
    "\n",
    "$$Y = XW = X[W_1, W_2, ..., W_n] = [XW_1, XW_2, ..., XW_n]$$\n",
    "\n",
    "![](./images/Code02Megatron01.png)\n",
    "\n",
    "对于反向传播，梯度也需要相应的分割和聚合操作。这种并行策略特别适合 Transformer 架构，因为其核心组件（MLP 和 Attention）都包含大量的矩阵运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8791721",
   "metadata": {},
   "source": [
    "下面我们先进行模型结构的搭建和初始化分布式训练环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yclhvmkucjr",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================\n",
    "全局配置参数（轻量级配置，快速验证）\n",
    "============================================\n",
    "\"\"\"\n",
    "\n",
    "# 模型配置\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': 1024,        # 词汇表大小（轻量级）\n",
    "    'hidden_size': 512,       # 隐藏层维度\n",
    "    'num_layers': 8,          # Transformer 层数\n",
    "    'num_heads': 8,           # 注意力头数（需被 NUM_GPUS 整除）\n",
    "    'ffn_size': 2048,         # MLP 中间层维度（通常为 hidden_size 的 4 倍）\n",
    "}\n",
    "\n",
    "# 训练配置（序列记忆任务）\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 8,          # 批次大小\n",
    "    'seq_length': 32,         # 序列长度\n",
    "    'num_epochs': 5,          # 训练轮数\n",
    "    'lr': 1e-3,               # 学习率\n",
    "    'print_interval': 10,     # 打印间隔（steps）\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Megatron 张量并行验证\n",
    "- 测试Megatron-LM的分布式训练\n",
    "- 更小的模型规模（快速验证）\n",
    "- 简单的序列记忆任务\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parameter import Parameter\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "# Check and warn about import issues\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA is not available. This script requires CUDA.\", file=sys.stderr)\n",
    "\n",
    "def init_distributed():\n",
    "    \"\"\"初始化分布式环境\"\"\"\n",
    "    if not dist.is_available():\n",
    "        raise RuntimeError(\"Distributed package is not available.\")\n",
    "\n",
    "    # Set NCCL environment variables\n",
    "    os.environ[\"NCCL_DEBUG\"] = os.environ.get(\"NCCL_DEBUG\", \"WARN\")\n",
    "    os.environ[\"NCCL_SOCKET_IFNAME\"] = os.environ.get(\"NCCL_SOCKET_IFNAME\", \"^docker0,lo\")\n",
    "    os.environ[\"NCCL_IB_DISABLE\"] = os.environ.get(\"NCCL_IB_DISABLE\", \"1\")\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = os.environ.get(\"NCCL_P2P_DISABLE\", \"0\")\n",
    "    # 使用PyTorch推荐的环境变量名\n",
    "    os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = os.environ.get(\"TORCH_NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "    print(f\"Rank: {rank}, World size: {world_size}\")\n",
    "\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    import datetime\n",
    "    timeout_minutes = int(os.environ.get(\"TORCH_DIST_TIMEOUT_MINUTES\", \"30\"))\n",
    "\n",
    "    try:\n",
    "        dist.init_process_group(\n",
    "            backend=backend,\n",
    "            init_method=\"env://\",\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            timeout=datetime.timedelta(minutes=timeout_minutes)\n",
    "        )\n",
    "        print(f\"Rank {rank}: Successfully initialized with {backend} backend\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rank {rank}: Failed to initialize: {str(e)}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", rank % torch.cuda.device_count()))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return dist.get_rank(), dist.get_world_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a832685",
   "metadata": {},
   "source": [
    "构建一些基本的工具函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllGather(torch.autograd.Function):\n",
    "    \"\"\"All-Gather 操作 - 在特征维度上拼接各GPU的部分输出\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.world_size = dist.get_world_size()\n",
    "        gathered = [torch.zeros_like(x) for _ in range(ctx.world_size)]\n",
    "        dist.all_gather(gathered, x)\n",
    "        return torch.cat(gathered, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad.chunk(ctx.world_size, dim=-1)[dist.get_rank()]\n",
    "\n",
    "class AllReduce(torch.autograd.Function):\n",
    "    \"\"\"AllReduce操作的autograd包装 - 修复PyTorch警告\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = x.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        # 梯度在反向传播时也需要all_reduce\n",
    "        output = grad.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f5178",
   "metadata": {},
   "source": [
    "这些基础工具函数为 TP 提供了必要的通信原语，两者均支持自动微分，确保反向传播时梯度能正确传递。\n",
    "\n",
    "## 2. MLP 层 TP 实现\n",
    "\n",
    "在 Transformer 的 MLP 层中，通常包含两个线性变换和一个激活函数：\n",
    "\n",
    "$$MLP(x) = Activation(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    " TP 将这两个线性变换分割到多个设备上，核心策略是**列并行+行并行**的组合，平衡计算量与通信开销：\n",
    "\n",
    "1. 第一个线性变换（$xW_1$）按列分割权重 $W_1$，每个设备计算部分输出后通过 All-Gather 聚合；\n",
    "2. 第二个线性变换（$Activation(...)W_2$）按行分割权重 $W_2$，输入先通过 Reduce-Scatter 分散后再计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30531c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnLinear(nn.Module):\n",
    "    \"\"\"列并行线性层\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_out_dim = out_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(self.local_out_dim, in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(self.local_out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_out = F.linear(x, self.weight, self.bias)\n",
    "        return local_out\n",
    "\n",
    "class RowLinear(nn.Module):\n",
    "    \"\"\"行并行线性层 - 使用AllReduce包装\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_in_dim = in_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(out_dim, self.local_in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_chunks = x.chunk(dist.get_world_size(), dim=-1)\n",
    "        local_input = input_chunks[dist.get_rank()]\n",
    "        local_output = F.linear(local_input, self.weight, self.bias)\n",
    "        # 使用AllReduce包装，修复autograd警告\n",
    "        return AllReduce.apply(local_output)\n",
    "\n",
    "class ParallelMLP(nn.Module):\n",
    "    \"\"\"并行 MLP 层\"\"\"\n",
    "    def __init__(self, hidden_size, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.fc1 = ColumnLinear(hidden_size, ffn_size, world_size, rank)\n",
    "        self.fc2 = RowLinear(ffn_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate = self.fc1(x)\n",
    "        intermediate_full = AllGather.apply(intermediate)\n",
    "        activated = F.gelu(intermediate_full)\n",
    "        return self.fc2(activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1e55f",
   "metadata": {},
   "source": [
    "该实现的核心优势是**无计算冗余**：每个设备仅计算部分矩阵乘法，通过两次通信操作（All-Gather+Reduce-Scatter）确保最终结果与单卡计算完全一致，同时将单卡内存占用降低至 $1/world_size$。\n",
    "\n",
    "![](./images/Code02Megatron03.png)\n",
    "\n",
    "## 3. Attention 层 TP 实现\n",
    "\n",
    "Transformer 的 Attention 层包含三个核心计算：Q（查询）、K（键）、V（值）的投影，以及 Attention 分数计算与加权求和。其数学表达为：\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "在 TP 中，核心策略是**注意力头分片**：将所有注意力头均匀分配到多个设备，每个设备仅计算部分头的 Attention 结果，最后通过输出投影层聚合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9da4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelAttention(nn.Module):\n",
    "    \"\"\"并行 Attention 层\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, world_size, rank):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert num_heads % world_size == 0\n",
    "\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.world_size = world_size\n",
    "        self.local_heads = num_heads // world_size\n",
    "\n",
    "        self.q_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.k_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.v_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.out_proj = RowLinear(hidden_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores += mask\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, S, self.local_heads * self.head_dim)\n",
    "        complete_attn_output = AllGather.apply(attn_output)\n",
    "\n",
    "        return self.out_proj(complete_attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cc6d2",
   "metadata": {},
   "source": [
    "该实现的关键设计是**头级并行**：每个设备仅存储部分 Q/K/V 投影权重，计算部分注意力头，避免了全量 Attention 计算的内存开销。\n",
    "\n",
    "![](./images/Code02Megatron02.png)\n",
    "\n",
    "## 4. 完整并行 Transformer\n",
    "\n",
    "完整的 Transformer 块包含“多头注意力层+MLP 层”，并配合残差连接和层归一化。在 TP 中，层归一化需在所有设备上**独立同步执行**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTransformerBlock(nn.Module):\n",
    "    \"\"\"并行 Transformer 块\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.attn = ParallelAttention(hidden_size, num_heads, world_size, rank)\n",
    "        self.mlp = ParallelMLP(hidden_size, ffn_size, world_size, rank)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580eb51b",
   "metadata": {},
   "source": [
    "## 5. Embedding 层并行\n",
    "\n",
    "在大型语言模型中，词汇表规模常达数万至数十万，导致嵌入层占用大量内存。Embedding Parallel 通过**词汇表分片**解决这一问题：每个设备仅保存部分词嵌入，通过掩码和 All-Gather 聚合完整结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    \"\"\"并行 Embedding 层\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        part_size = vocab_size // world_size\n",
    "        remainder = vocab_size % world_size\n",
    "        self.start_idx = rank * part_size + min(rank, remainder)\n",
    "        self.end_idx = self.start_idx + part_size + (1 if rank < remainder else 0)\n",
    "        self.local_vocab_size = self.end_idx - self.start_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(self.local_vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        local_input = input.clone() - self.start_idx\n",
    "        mask = (input >= self.start_idx) & (input < self.end_idx)\n",
    "        local_input[~mask] = 0\n",
    "\n",
    "        local_emb = self.embedding(local_input)\n",
    "        local_emb[~mask] = 0\n",
    "\n",
    "        # 使用AllReduce包装\n",
    "        return AllReduce.apply(local_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8837d2b",
   "metadata": {},
   "source": [
    "## 6. 完整 Transformer 并行\n",
    "\n",
    "将上述并行组件（ EP 嵌入、Transformer 并行、并行输出层）组合，形成完整的 TP。输出层采用列并行，确保与 EP 的分割策略一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTransformer(nn.Module):\n",
    "    \"\"\"完整的并行 Transformer 模型\"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.embedding = ParallelEmbedding(vocab_size, hidden_size, world_size, rank)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1024, hidden_size))\n",
    "        self.layers = nn.ModuleList([\n",
    "            ParallelTransformerBlock(hidden_size, num_heads, ffn_size, world_size, rank)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.head = ColumnLinear(hidden_size, vocab_size, world_size, rank)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) + self.pos_embed[:, :input_ids.size(1)]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        local_logits = self.head(self.norm(x))\n",
    "        return AllGather.apply(local_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843f2cb",
   "metadata": {},
   "source": [
    "## 7. 实验与性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aed971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_stats():\n",
    "    \"\"\"获取当前GPU显存使用情况（MB）\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2    # MB\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        return {\n",
    "            'allocated': allocated,\n",
    "            'reserved': reserved,\n",
    "            'max_allocated': max_allocated\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def reset_memory_stats():\n",
    "    \"\"\"重置显存统计\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def create_sequence_memory_task(vocab_size=512, seq_len=32, num_sequences=100):\n",
    "    \"\"\"\n",
    "    创建简单的序列记忆任务\n",
    "    - 有限词汇表（512个token）\n",
    "    - 短序列（32个token）\n",
    "    - 固定的训练序列（100条）\n",
    "    \"\"\"\n",
    "    # 生成固定的训练序列，确保可重复\n",
    "    torch.manual_seed(42)\n",
    "    sequences = []\n",
    "    for i in range(num_sequences):\n",
    "        # 每个序列有一定的模式，更容易学习\n",
    "        seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "def train_example():\n",
    "    \"\"\"训练示例\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "    try:\n",
    "        rank, world_size = init_distributed()\n",
    "\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        if gpu_count < world_size:\n",
    "            raise RuntimeError(f\"Not enough GPUs. Required: {world_size}, Available: {gpu_count}\")\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Megatron 张量并行验证\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"GPU数量: {world_size}\")\n",
    "            print(f\"主机名: {socket.gethostname()}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # 更小的模型配置 - 用于快速验证\n",
    "        config = {\n",
    "            'vocab_size': 1024,\n",
    "            'hidden_size': 512,\n",
    "            'num_layers': 8,\n",
    "            'num_heads': 8,\n",
    "            'ffn_size': 1024,\n",
    "        }\n",
    "\n",
    "        model = ParallelTransformer(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ffn_size=config['ffn_size'],\n",
    "            world_size=world_size,\n",
    "            rank=rank\n",
    "        ).cuda()\n",
    "\n",
    "        if rank == 0:\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"\\n模型配置:\")\n",
    "            print(f\"  - Vocab: {config['vocab_size']}, Hidden: {config['hidden_size']}\")\n",
    "            print(f\"  - Layers: {config['num_layers']}, Heads: {config['num_heads']}\")\n",
    "            print(f\"  - 参数量: {total_params:,} (每GPU约 {total_params//(world_size*4):,})\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # 优化器 - 使用更大的学习率\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        # 创建简单的序列记忆任务\n",
    "        train_data = create_sequence_memory_task(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            seq_len=32,  # 短序列\n",
    "            num_sequences=100  # 100条训练序列\n",
    "        )\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n训练任务: 序列记忆\")\n",
    "            print(f\"  - 训练序列数: {train_data.shape[0]}\")\n",
    "            print(f\"  - 序列长度: {train_data.shape[1]}\")\n",
    "            print(f\"  - 词汇表大小: {config['vocab_size']}\")\n",
    "            print(f\"\\n开始训练...\")\n",
    "            print(f\"{'-'*60}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # 重置显存统计\n",
    "        reset_memory_stats()\n",
    "\n",
    "        # 训练循环 - 多个epoch确保收敛\n",
    "        num_epochs = 5\n",
    "        steps_per_epoch = 100\n",
    "        best_loss = float('inf')\n",
    "        peak_memory = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for step in range(steps_per_epoch):\n",
    "                # 从训练集随机采样\n",
    "                batch_size = 16\n",
    "                indices = torch.randint(0, len(train_data), (batch_size,))\n",
    "                input_ids = train_data[indices].cuda()\n",
    "\n",
    "                # 前向传播\n",
    "                logits = model(input_ids)\n",
    "\n",
    "                # 计算损失\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, config['vocab_size']),\n",
    "                    input_ids.view(-1)\n",
    "                )\n",
    "\n",
    "                # 反向传播\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # 更新峰值显存\n",
    "                if torch.cuda.is_available():\n",
    "                    current_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "                    peak_memory = max(peak_memory, current_mem)\n",
    "\n",
    "                # 打印进度\n",
    "                if rank == 0 and step % 20 == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Step {step:3d}/{steps_per_epoch}, Loss: {loss.item():.4f}, Avg: {avg_loss:.4f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / steps_per_epoch\n",
    "\n",
    "            if rank == 0:\n",
    "                improvement = \"\" if epoch == 0 else f\" (↓{best_loss - avg_epoch_loss:.4f})\"\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Epoch {epoch+1} 完成 - 平均Loss: {avg_epoch_loss:.4f}{improvement}\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "                if avg_epoch_loss < best_loss:\n",
    "                    best_loss = avg_epoch_loss\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"✅ 训练完成!\")\n",
    "            print(f\"   最佳Loss: {best_loss:.4f}\")\n",
    "            print(f\"   最终Loss: {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   Loss下降: {train_data.shape[0] * 0.1:.4f} → {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   峰值显存: {peak_memory:.2f} MB ({peak_memory/1024:.2f} GB)\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        print(f\"Rank {error_rank} encountered error: {str(e)}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()\n",
    "        raise e\n",
    "    finally:\n",
    "        if dist.is_initialized():\n",
    "            try:\n",
    "                dist.barrier()\n",
    "            except:\n",
    "                pass\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fa306",
   "metadata": {},
   "source": [
    "\n",
    "这里我们先运行一个jupyter的命令，提取jupyter文件中的Python代码块到一个新的python文件当中:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to python Code02Megatron.ipynb\n",
    "```\n",
    "\n",
    "由于jupyter文件不适合运行多线程，也就不适合进行分布式训练，这里提供torchrun脚本进行分布式训练。\n",
    "\n",
    "以下是运行训练的bash脚本：\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "set -e\n",
    "\n",
    "echo \"==============================================\"\n",
    "echo \"4GPU张量并行测试\"\n",
    "echo \"==============================================\"\n",
    "\n",
    "# 配置参数\n",
    "NUM_GPUS=4\n",
    "MASTER_PORT=29501\n",
    "\n",
    "echo \"\"\n",
    "echo \"配置信息:\"\n",
    "echo \"  - GPU数量: $NUM_GPUS\"\n",
    "echo \"  - Master端口: $MASTER_PORT\"\n",
    "echo \"  - 模式: Megatron张量并行\"\n",
    "echo \"  - 用途: 显存占用对比测试\"\n",
    "echo \"\"\n",
    "\n",
    "# 检查GPU可用性\n",
    "echo \"检查GPU状态...\"\n",
    "nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "echo \"\"\n",
    "\n",
    "# 启动分布式训练\n",
    "echo \"启动4GPU并行训练...\"\n",
    "echo \"==============================================\"\n",
    "torchrun \\\n",
    "    --nproc_per_node=$NUM_GPUS \\\n",
    "    --master_port=$MASTER_PORT \\\n",
    "    Code02Megatron.py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362924a",
   "metadata": {},
   "source": [
    "以下是在4个GPU的分布式环境下执行后的最终输出：\n",
    "\n",
    "```\n",
    "==============================================\n",
    "4GPU张量并行测试\n",
    "==============================================\n",
    "\n",
    "配置信息:\n",
    "  - GPU数量: 4\n",
    "  - Master端口: 29501\n",
    "  - 模式: Megatron张量并行\n",
    "  - 用途: 显存占用对比测试\n",
    "\n",
    "检查GPU状态...\n",
    "index, name, memory.total [MiB]\n",
    "0, NVIDIA GeForce RTX 2080 Ti, 11264 MiB\n",
    "1, NVIDIA GeForce RTX 2080 Ti, 11264 MiB\n",
    "2, NVIDIA GeForce RTX 2080 Ti, 11264 MiB\n",
    "3, NVIDIA GeForce RTX 2080 Ti, 11264 MiB\n",
    "\n",
    "启动4GPU并行训练...\n",
    "==============================================\n",
    "Rank: 1, World size: 4\n",
    "Rank: 0, World size: 4\n",
    "Rank: 2, World size: 4\n",
    "Rank: 3, World size: 4\n",
    "Rank 1: Successfully initialized with nccl backend\n",
    "Rank 3: Successfully initialized with nccl backend\n",
    "Rank 0: Successfully initialized with nccl backend\n",
    "Rank 2: Successfully initialized with nccl backend\n",
    "[rank1]:[W1105 13:37:10.134661876 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
    "\n",
    "============================================================\n",
    "Megatron 张量并行验证\n",
    "============================================================\n",
    "GPU数量: 4\n",
    "主机名: autodl-container-fa394eb8f4-296cf614\n",
    "[rank3]:[W1105 13:37:10.148326282 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
    "[rank2]:[W1105 13:37:10.148474172 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
    "[rank0]:[W1105 13:37:10.148670612 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
    "NCCL version 2.21.5+cuda12.4\n",
    "\n",
    "模型配置:\n",
    "  - Vocab: 1024, Hidden: 512\n",
    "  - Layers: 8, Heads: 8\n",
    "  - 参数量: 7,110,912 (每GPU约 444,432)\n",
    "\n",
    "训练任务: 序列记忆\n",
    "  - 训练序列数: 100\n",
    "  - 序列长度: 32\n",
    "  - 词汇表大小: 1024\n",
    "\n",
    "开始训练...\n",
    "------------------------------------------------------------\n",
    "Epoch 1/5, Step   0/100, Loss: 7.6680, Avg: 7.6680\n",
    "Epoch 1/5, Step  20/100, Loss: 2.1710, Avg: 4.2445\n",
    "Epoch 1/5, Step  40/100, Loss: 0.0844, Avg: 2.4003\n",
    "Epoch 1/5, Step  60/100, Loss: 0.0101, Avg: 1.6213\n",
    "Epoch 1/5, Step  80/100, Loss: 0.0070, Avg: 1.2238\n",
    "============================================================\n",
    "Epoch 1 完成 - 平均Loss: 0.9922\n",
    "============================================================\n",
    "\n",
    "Epoch 2/5, Step   0/100, Loss: 0.0049, Avg: 0.0049\n",
    "Epoch 2/5, Step  20/100, Loss: 0.0036, Avg: 0.0041\n",
    "Epoch 2/5, Step  40/100, Loss: 0.0030, Avg: 0.0037\n",
    "Epoch 2/5, Step  60/100, Loss: 0.0028, Avg: 0.0033\n",
    "Epoch 2/5, Step  80/100, Loss: 0.0022, Avg: 0.0031\n",
    "============================================================\n",
    "Epoch 2 完成 - 平均Loss: 0.0029 (↓0.9893)\n",
    "============================================================\n",
    "\n",
    "Epoch 3/5, Step   0/100, Loss: 0.0021, Avg: 0.0021\n",
    "Epoch 3/5, Step  20/100, Loss: 0.0018, Avg: 0.0019\n",
    "Epoch 3/5, Step  40/100, Loss: 0.0016, Avg: 0.0018\n",
    "Epoch 3/5, Step  60/100, Loss: 0.0016, Avg: 0.0017\n",
    "Epoch 3/5, Step  80/100, Loss: 0.0014, Avg: 0.0017\n",
    "============================================================\n",
    "Epoch 3 完成 - 平均Loss: 0.0016 (↓0.0013)\n",
    "============================================================\n",
    "\n",
    "Epoch 4/5, Step   0/100, Loss: 0.0013, Avg: 0.0013\n",
    "Epoch 4/5, Step  20/100, Loss: 0.0012, Avg: 0.0012\n",
    "Epoch 4/5, Step  40/100, Loss: 0.0011, Avg: 0.0012\n",
    "Epoch 4/5, Step  60/100, Loss: 0.0011, Avg: 0.0011\n",
    "Epoch 4/5, Step  80/100, Loss: 0.0009, Avg: 0.0011\n",
    "============================================================\n",
    "Epoch 4 完成 - 平均Loss: 0.0011 (↓0.0005)\n",
    "============================================================\n",
    "\n",
    "Epoch 5/5, Step   0/100, Loss: 0.0009, Avg: 0.0009\n",
    "Epoch 5/5, Step  20/100, Loss: 0.0009, Avg: 0.0009\n",
    "Epoch 5/5, Step  40/100, Loss: 0.0008, Avg: 0.0009\n",
    "Epoch 5/5, Step  60/100, Loss: 0.0008, Avg: 0.0008\n",
    "Epoch 5/5, Step  80/100, Loss: 0.0007, Avg: 0.0008\n",
    "============================================================\n",
    "Epoch 5 完成 - 平均Loss: 0.0008 (↓0.0003)\n",
    "============================================================\n",
    "\n",
    "\n",
    "============================================================\n",
    "✅ 训练完成!\n",
    "   最佳Loss: 0.0008\n",
    "   最终Loss: 0.0008\n",
    "   Loss下降: 10.0000 → 0.0008\n",
    "   峰值显存: 244.33 MB (0.24 GB)\n",
    "============================================================\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2359c1",
   "metadata": {},
   "source": [
    "训练过程中，仅`rank=0`每 100 步打印损失值，模型从随机初始化的高损失逐渐下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe79b8",
   "metadata": {},
   "source": [
    "另外，单GPU情况下的训练输出为：\n",
    "\n",
    "```\n",
    "============================================================\n",
    "单GPU基准测试（无张量并行）\n",
    "============================================================\n",
    "使用设备: cuda:0\n",
    "\n",
    "模型配置:\n",
    "  - Vocab: 1024, Hidden: 512\n",
    "  - Layers: 8, Heads: 8\n",
    "  - 总参数量: 26,793,984\n",
    "\n",
    "训练任务: 序列记忆\n",
    "  - 训练序列数: 100\n",
    "  - 序列长度: 32\n",
    "  - 词汇表大小: 1024\n",
    "\n",
    "开始训练...\n",
    "...\n",
    "============================================================\n",
    "✅ 训练完成!\n",
    "   最佳Loss: 0.0023\n",
    "   最终Loss: 0.0023\n",
    "   Loss下降: 10.0000 → 0.0023\n",
    "   峰值显存: 564.16 MB (0.55 GB)\n",
    "============================================================\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3effcb",
   "metadata": {},
   "source": [
    "TP 的核心优势是**降低单卡内存占用**。以`world_size=4`为例，对比单卡训练与 4 卡 TP 的内存峰值：\n",
    "\n",
    "| 训练模式       | 单卡内存占用（峰值） | 2 卡 TP 单卡内存占用（峰值） | 内存节省比例 |\n",
    "|----------------|----------------------|---------------------------------|--------------|\n",
    "| 无并行（单卡） | ~564MB                | -                               | -            |\n",
    "| 4 卡 TP     | -                    | ~244MB                             | ~56.7%       |\n",
    "\n",
    "## 总结与思考\n",
    "\n",
    "本实验通过可执行的代码深入探讨了 Megatron 风格的 TP 原理与实现，并验证了 TP 在内存节省上的有效性。\n",
    "\n",
    "**核心技术点**：\n",
    "- **列并行线性层**：将权重矩阵按列分割，前向传播需要 All-Gather 操作\n",
    "- **行并行线性层**：将权重矩阵按行分割，前向传播使用 AllReduce 聚合（已优化autograd）\n",
    "- **并行 Attention**：将注意力头分布到多个设备，每个设备处理部分头\n",
    "- **并行 Embedding**：将大型词汇表分割到多个设备，减少单个设备的内存压力\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
